1. 
You are training a classification model with logistic
regression. Which of the following statements are true? Check
all that apply.

Adding many new features to the model helps prevent overfitting on the training set.

Introducing regularization to the model always results in equal or better performance on 
examples not in the training set.

X Adding a new feature to the model always results in equal or better performance on the 
training set.

Introducing regularization to the model always results in equal or better performance 
on the training set.

2. 
Suppose you ran logistic regression twice, once with λ=0, and once with λ=1. 
One of the times, you got parameters θ=[81.47; 12.69], and the other time you got
θ=[13.01; 0.91]. However, you forgot which value of λ corresponds to which value of θ. 
Which one do you think corresponds to λ=1?

X θ=[13.01; 0.91]
(bc values of \theta_1 decrease)

θ=[81.47; 12.69]

3.
Which of the following statements about regularization are true? Check all that apply.

Because logistic regression outputs values 0 \leq h_θ(x) \leq 1, it's range of output 
values can only be "shrunk" slightly by regularization anyway, so regularization is 
generally not helpful for it.

Using a very large value of λ cannot hurt the performance of your hypothesis; the only 
reason we do not set λ to be too large is to avoid numerical problems.

X Consider a classification problem. Adding regularization may cause your classifier to 
incorrectly classify some training examples (which it had correctly classified when not 
using regularization, i.e. when λ=0).

X Using too large a value of λ can cause your hypothesis to overfit the data; this can be 
avoided by reducing λ.

Because regularization causes J(θ) to no longer be convex, gradient descent may not 
always converge to the global minimum (when λ>0, and when using an appropriate learning 
rate α).

X Using too large a value of λ can cause your hypothesis to underfit the data.

4. 
In which one of the following figures do you think the hypothesis has overfit the training set?

X Figure:

Figure:

Figure:

Figure:


5. 
In which one of the following figures do you think the hypothesis has underfit the training set?

X Figure:

Figure:

Figure:

Figure:


